!: θέλει αλλαγή από 1η εργασία
~: θα μάθουμε τη τετάρτη

## PROJECT K23A Εργασία 2
	Χίου Ρίτα Άννα sdi1700192
	Χουσιανίτη Κατερίνα sdi1700194
	Μπούρα Τατιάνα sdi1700100

## Αρχεία και οργάνωση
Η οργάνωση των αρχείων έχει γίνει ως εξής: η κάθε χρησιμοποιούμενη δομή έχει τον δικό της φάκελο(hash_table,list και tuples), τα datasets βρίσκονται σε δικό τους φάκελο(datasets), ό,τι σχετίζεται με το reading και η main βρίσκονται στον φάκελο reading, τα unit tests στον unit_testing, ενώ παρέχεται και makefile και το παρόν README.md τα οποία δε βρίσκονται σε κάποιο φάκελο.  
Έχει φτιαχτεί και ένα workflow. Το execution time της εργασίας μας σε Ubuntu Latest φαίνεται στο βήμα 'runit' του job.

## Compile and run
Για να τρέξει κατευθείαν από το repository στο Project2, χρησιμοποιώντας τα αρχεία στο φάκελο dataset,
> make  
> ./prog ../datasets/camera_specs/2013_camera_specs/ ../datasets/sigmod_large_labelled_dataset.csv common-english-words.txt
					ή  
> ./prog ../datasets/camera_specs/2013_camera_specs/ ../datasets/sigmod_medium_labelled_dataset.csv common-english-words.txt,  
όπου sigmod_large_labelled_dataset.csv είναι το datasetW και το sigmod_medium_labelled_dataset το datasetY.  
  
Για το unit testing κάνουμε
> make tests  

όπου θα προκύψουν 6 εκτελέσιμα: 
- για τη δομή list
- για τη δομη hash_table όπου αποθηκεύονται όλες οι πληροφορίες κάθε .json αρχείου
- για τη δομη hash_table_pair που χρησιμοποιείται για τη δημιουργία των x_array, y_array για το training
- για τις δομές του vocabulary (hashTableVOC και λίστα)
- για τις συναρτήσεις που αφορούν τον υπολογισμό των tfidf, bag of words και τον επαναπροσδιορισμό του vocabulary
- για τη δομή wordInfo που χρησιμοποιείται για την αποθήκευση του περιεχομένου (λέξεων/αριθμού εμφανίσεων) του κάθε .json αρχείου
Για τα επιμέρους arguments του κάθε εκτελέσιμου ανατρέξατε στο TEST_LIST του κάθε .c του testing.

## Λειτουργικότητα / Σχεδιαστικές επιλογές

**!Generic/Sorted List**
Χρησιμοποιήσαμε μια generic λίστα καθώς παρατηρήσαμε πως ήταν απαραίτητη σε παραπάνω από μία υλοποιήσεις, όπως την αποθήκευση των ζευγών <όνομα_ιδιότητας,generic_list(τιμή_ιδιότητας)> (τα οποία ονομάζουμε tuples), τη δημιουργία κλικών από spec_ids και την διαχείριση του collision στα hashtable.
H append είναι ουσιαστικά put_at_front εφόσον δεν μας ενδιαφέρει η σειρά, αφού τότε έχει πολυπλοκότητα Ο(1).

**!Είδη Hash Table**
Αποφασίσαμε ότι η πολυπλοκότητα για την πρόσβαση στο εκάστοτε spec_id είναι ιδιαίτερα σημαντική λόγω του πλήθους ζευγών που ελέγχονται κατά την ανάγνωση των dataset W,Y. Επομένως επιλέξαμε να αποθηκεύσουμε τα specs (spec_id,tuples,cliques) σε ένα hash table που κάθε στοιχείο του table δείχνει σε μία λίστα από buckets, τα οποία buckets αποθηκεύουν εγγραφές. Έτσι, κάθε εγγραφή σε κάθε bucket αποτελείται από το spec_id, μια λίστα με ζεύγη <propertyName, λίστα από τιμές propertyValueList (περισσότερα του ενός στοιχεία σε περίπτωση πίνακα)> και μία λίστα από spec_ids που δηλώνει την κλίκα των spec που ταιριάζουν. 
Το μέγεθος του hash καθορίζεται από το συνολικό πλήθος των καμερών, καθώς στην αρχή του προγράμματος μετράμε τους φακέλους και το πόσες κάμερες έχουν μέσα, διατηρώντας τον συνολικό αριθμό.

**!Διάβασμα από dataset X**
Το πρόγραμμα για να εισάγει τα δεδομένα από το dataset X στις δομές που έχουμε επιλέξει σχεδιαστικά ακολουθεί την εξής διαδικασία:
Ανοίγει κάθε έναν από τους καταλόγους που αντιστοιχούν στις ιστοσελίδες διαδικτυακών καταστημάτων και επεξεργάζεται σειριακά όλα τα αρχεία .json που περιέχονται σε αυτούς.
Ολοκληρώνοντας την ανάγνωση του dataset X κάθε αρχείο .json έχει μετατραπεί σε μία λίστα ζευγών <όνομα_ιδιότητας,generic_list(τιμή_ιδιότητας)> (tuples) και έχει εισαχθεί στην δομή Hash Table με κλειδί της μορφής: 					<όνομα_ιστοτόπου>//<όνομα_αρχείου_χωρίς_την_κατάληξη_.json> 
έτσι ώστε να υποστηρίζει την μετέπειτα λειτουργία του προγράμματος.

Πιο συγκεκριμένα, για την μετατροπή κάθε αρχείου .json στη μορφή λίστας, το πρόγραμμα διαβάζει το αρχείο γραμμή-γραμμή, με χρήση της συνάρτησης fgets(char *str, int n, FILE *stream), και στη συνέχεια ελέγχοντας το περιεχόμενο του buffer(str) συνεχίζει σε μία από τις παρακάτω ενέργειες:

1)Αν περιέχει τους χαρακτήρες '{' η '}', οι οποίοι και οριοθετούν τα περιεχόμενα του αρχείου, το πρόγραμμα προχωράει στην ανάγνωση της επόμενης γραμμής του αρχείου.

2)Αν περιέχει τον χαρακτήρα '[' και μετά ακολουθεί αλλαγή γραμμής '\n', τότε έχει εντοπιστεί εγγραφή στο αρχείο με την μορφή πίνακα.
Σε αυτή την περίπτωση το πρόγραμμα συνεχίζει την ανάγνωση και αποθηκεύει κάθε γραμμή του αρχείου που αφορά το συγκεκριμένο ζεύγος (όνομα_ιδιότητας,τιμή_ιδιότητας) σε βοηθητικό buffer χρησιμοποιώντας τον ειδικό χαρακτήρα '#' για τον διαχωρισμό των γραμμών. Το τέλος ενός πίνακα σηματοδοτείται από από τον εντοπισμό γραμμής που περιέχει μόνο την αλληλουχία χαρακτήρων ']'και',' ή ']'και'\n'. Στην συνέχεια καλεί την συνάρτηση  json_array_handler(char* str, TuplePtr t) στην οποία το όρισμα str αποτελεί μία συμβολοσειρά της μορφής "όνομα_ιδιότητας#τιμή_ιδιότητας_1#τιμή_ιδιότητας_2#...#τιμή_ιδιότητας_n#". H συνάρτηση αυτή διαχωρίζει  κατάλληλα τη συμβολοσειρά str και αρχικοποιεί την δομή TuplePtr t περνώντας το όνομα_ιδιότητας και εισάγωντας διαδοχικά τις τιμές ιδιότητας στην generic λίστα της δομής.

3)Αν δεν ισχύει κάποια από τις περιπτώσεις 1) και 2) τότε το buffer περιέχει μία γραμμή στην οποία υπάρχει ένα ζεύγος (όνομα_ιδιότητας,τιμή_ιδιότητας).Τότε καλείται η συνάρτηση json_separator(char* buff,TuplePtr t) η οποία διαχωρίζει  κατάλληλα τη συμβολοσειρά str και αρχικοποιεί την δομή TuplePtr t περνώντας το όνομα_ιδιότητας και εισάγωντας την τιμή ιδιότητας στην generic λίστα της δομής. 

**!Διαχείση μη ταιριασμάτων**
Όταν διαβάσουμε από το dataset W ένα ζεύγος left_spec_id, right_spec_id τα όποια είναι ίδια (έχουν δηλαδή label να ισούται με 1), hash-άρουμε το κάθε προαναφερθέν id, βρίσκουμε σε ποιά θέση του hash table βρίσκεται και ύστερα με τη συνάρτηση foundInHT επιστρέφουμε το bucket στο οποίο ανήκει το συγκεκριμένο id καθώς και τη θέση που καταλαμβάνει στο bucket(δηλαδή ποιά εγγραφή είναι). Αφότου έχουμε τις ακριβείς θέσεις και των 2 spec id, καλούμε τη συνάρτηση changePointers η οποία αφότου ελέγξει ότι δεν αναφερόμαστε στην ίδια κλίκα(δηλαδή αποφεύγει φαινόμενα κυκλισμού), κάνει merge τις δύο κλίκες (λίστες) και σε κάθε στοιχείο της merged πια κλίκας πηγαίνει και αλλάζει τον pointer του προκειμένου όλα τα στοιχεία της κλίκας να δείχνουν(άρα και να αναφέρονται) στην ίδια κλίκα. 

**~Εκτύπωση αποτελέσματος**
Η εκτύπωση στο output.txt γίνεται όπως ακριβώς και στην πρώτη εργασία. Δημιουργείται επιπλέον ένα ακόμη αρχείο για τα αποτελέσματα του training με το όνομα result.txt. Κάθε γραμμή του περιέχει μια λέξη του vocabulary συνοδευόμενη από το τελικό βάρος της που προέκυψε μετά το training.

**Unit Testing**
Εφαρμόζουμε unit testing στις λίστες, τα τρία είδη hashTable, το wordInfo structure, και στις κύριες συναρτήσεις για τη δημιουργία των απαραίτητων για το training δομών. Οι μόνες συναρτήσεις που δεν ελέγχονται από το testing είναι μερικές από τις εκείνες που ασχολούνται με το input-output του προγράμματος, αυτές που αποδεσμεύουν μνήμη(αν και το πρόγραμμα δεν έχει κανένα leak ή error σύμφωνα με το Valgrind) και κάποιες μικρές συναρτήσεις εντός των δομών που ελέγχονται έμμεσα, αφού αποτελούν τμήμα μεγαλύτερων συναρτήσεων που ελέγχονται κανονικά. 
