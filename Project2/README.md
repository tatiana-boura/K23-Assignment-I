!: θέλει αλλαγή από 1η εργασία
~: θα μάθουμε τη τετάρτη

## PROJECT K23A Εργασία 2
	Χίου Ρίτα Άννα sdi1700192
	Χουσιανίτη Κατερίνα sdi1700194
	Μπούρα Τατιάνα sdi1700100

## Αρχεία και οργάνωση
Η οργάνωση των αρχείων έχει γίνει ως εξής: η κάθε χρησιμοποιούμενη δομή έχει τον δικό της φάκελο(hash_table,list και tuples), τα datasets βρίσκονται σε δικό τους φάκελο(datasets), ό,τι σχετίζεται με το reading και η main βρίσκονται στον φάκελο reading, τα unit tests στον unit_testing, ενώ παρέχεται και makefile και το παρόν README.md τα οποία δε βρίσκονται σε κάποιο φάκελο.  
Έχει φτιαχτεί και ένα workflow. Το execution time της εργασίας μας σε Ubuntu Latest φαίνεται στο βήμα 'runit' του job.

## Compile and run
Για να τρέξει κατευθείαν από το repository στο Project2, χρησιμοποιώντας τα αρχεία στο φάκελο dataset,
> make  
> ./prog ../datasets/camera_specs/2013_camera_specs/ ../datasets/sigmod_large_labelled_dataset.csv common-english-words.txt
					ή  
> ./prog ../datasets/camera_specs/2013_camera_specs/ ../datasets/sigmod_medium_labelled_dataset.csv common-english-words.txt,  
όπου sigmod_large_labelled_dataset.csv είναι το datasetW και το sigmod_medium_labelled_dataset το datasetY.  
  
Για το unit testing κάνουμε
> make tests  

όπου θα προκύψουν 6 εκτελέσιμα: 
- για τη δομή list
- για τη δομη hash_table όπου αποθηκεύονται όλες οι πληροφορίες κάθε .json αρχείου
- για τη δομη hash_table_pair που χρησιμοποιείται για τη δημιουργία των x_array, y_array για το training
- για τις δομές του vocabulary (hashTableVOC και λίστα)
- για τις συναρτήσεις που αφορούν τον υπολογισμό των tfidf, bag of words και τον επαναπροσδιορισμό του vocabulary
- για τη δομή wordInfo που χρησιμοποιείται για την αποθήκευση του περιεχομένου (λέξεων/αριθμού εμφανίσεων) του κάθε .json αρχείου
Για τα επιμέρους arguments του κάθε εκτελέσιμου ανατρέξατε στο TEST_LIST του κάθε .c του testing.

## Λειτουργικότητα / Σχεδιαστικές επιλογές

**Generic/Sorted List**
Οι generic λίστες μας φάνηκαν χρήσιμες για τις ανάγκες της πρώτης εργασίας (για τη δημιουργία κλικών από spec_ids και την διαχείριση του collision στα hashtable), αλλά και τις αλλαγές που προέκυψαν στην δεύτερη: για την συλλογή λέξεων (vocabulary) και την αποθήκευση των μη-κλικών, πάντα διατηρώντας την append from front συμπεριφορά για αποτελεσματικότερη εισαγωγή. 
Η sorted list αποσκοπεί στην γρήγορη πρόσβαση λέξεων στη wordInfoList (στο entry κάθε json του hash table) προς αποφυγή διπλοτύπων κατά την εισαγωγή νέων λέξεων.

**Είδη Hash Table**
Σε αυτή την εργασία εντοπίσαμε την ανάγκη για 3 είδη hash table:
1. Η πρώτη μορφή είναι παρόμοια με αυτή της πρώτης εργασίας με τις εξής διαφορές:
- αντί για την λίστα από tuples (<όνομα_ιδιότητας,τιμές_ιδιότητας>), χρησιμοποιείται μια λίστα της δομής wordInfo (<λέξη,εμφανίσεις λέξης στο .json>) ώστε να μπορέσει αργότερα να υπολογιστεί το tfidf / bag of words
- προστέθηκε μια ακόμη λίστα, η notClique, η οποία αποτελεί λίστα από διευθύνσεις κλικών οι οποίες δεν ταιριάζουν με την κλίκα του συγκεκριμένου .json
- προστέθηκε ένας πίνακας από αριθμούς float που αποτελούν το tfidf / bag of words του .json ανάλογα με την επιλογή του χρήστη
2. Το δεύτερο hash table είναι αυτό που συγκρατεί τις θέσεις των λέξεων στο vocabulary και είναι τύπου hashTableVOC. Κάθε entry του αποτελείται από την λέξη, τη θέση της στη λίστα vocabulary και το στοιχείο wordInfo της λίστας.
3. Το τελευτείο είδος hash table που δημιουργήσαμε είναι το hashTablePair για την data_for_training. Αυτό το hash table αποτελείται από 2 spec_ids, τα οποία εισάγονται αφού χρησιμοποιηθούν ως ζεύγος στον πίνακα απόλυτων διαφορών για το training (x_array). Για την επιλογή επόμενων ζευγών, θα χρησιμοποιηθεί η foundInHTPair, η οποία θα ελέγξει αν τα 2 spec_ids υπάρχουν ήδη στο hash table (ως spec1,spec2 ή spec2,spec1).

**Δομή wordInfo**
Για τις ανάγκες υπολογισμού των tfidf / bag of words, δημιουργήθηκε η δομή wordInfo (<word,count>), η οποία αξιοποιείται στη δημιουργία του vocabulary και στη δομή hashTable. Η λίστα vocabulary, στην οποία αποθηκεύονται όλες οι λέξεις όλων των αρχείων .json, διαθέτει στοιχεία wordInfo* με word την εκάστοτε λέξη και count τον αριθμό των αρχείων στα οποία υπάρχει. Στο hashTable, η λίστα wordInfoList αποτελείται από στοιχεία wordInfo* ωστόσο εδώ το count φυλάει το πλήθος εμφανίσεων της λέξης στο συγκεκριμένο αρχείο. 

**!Διάβασμα από dataset X**
Το πρόγραμμα για να εισάγει τα δεδομένα από το dataset X στις δομές που έχουμε επιλέξει σχεδιαστικά ακολουθεί την εξής διαδικασία:
Ανοίγει κάθε έναν από τους καταλόγους που αντιστοιχούν στις ιστοσελίδες διαδικτυακών καταστημάτων και επεξεργάζεται σειριακά όλα τα αρχεία .json που περιέχονται σε αυτούς.
Ολοκληρώνοντας την ανάγνωση του dataset X κάθε αρχείο .json έχει μετατραπεί σε μία λίστα ζευγών <όνομα_ιδιότητας,generic_list(τιμή_ιδιότητας)> (tuples) και έχει εισαχθεί στην δομή Hash Table με κλειδί της μορφής: 					<όνομα_ιστοτόπου>//<όνομα_αρχείου_χωρίς_την_κατάληξη_.json> 
έτσι ώστε να υποστηρίζει την μετέπειτα λειτουργία του προγράμματος.

Πιο συγκεκριμένα, για την μετατροπή κάθε αρχείου .json στη μορφή λίστας, το πρόγραμμα διαβάζει το αρχείο γραμμή-γραμμή, με χρήση της συνάρτησης fgets(char *str, int n, FILE *stream), και στη συνέχεια ελέγχοντας το περιεχόμενο του buffer(str) συνεχίζει σε μία από τις παρακάτω ενέργειες:

1)Αν περιέχει τους χαρακτήρες '{' η '}', οι οποίοι και οριοθετούν τα περιεχόμενα του αρχείου, το πρόγραμμα προχωράει στην ανάγνωση της επόμενης γραμμής του αρχείου.

2)Αν περιέχει τον χαρακτήρα '[' και μετά ακολουθεί αλλαγή γραμμής '\n', τότε έχει εντοπιστεί εγγραφή στο αρχείο με την μορφή πίνακα.
Σε αυτή την περίπτωση το πρόγραμμα συνεχίζει την ανάγνωση και αποθηκεύει κάθε γραμμή του αρχείου που αφορά το συγκεκριμένο ζεύγος (όνομα_ιδιότητας,τιμή_ιδιότητας) σε βοηθητικό buffer χρησιμοποιώντας τον ειδικό χαρακτήρα '#' για τον διαχωρισμό των γραμμών. Το τέλος ενός πίνακα σηματοδοτείται από από τον εντοπισμό γραμμής που περιέχει μόνο την αλληλουχία χαρακτήρων ']'και',' ή ']'και'\n'. Στην συνέχεια καλεί την συνάρτηση  json_array_handler(char* str, TuplePtr t) στην οποία το όρισμα str αποτελεί μία συμβολοσειρά της μορφής "όνομα_ιδιότητας#τιμή_ιδιότητας_1#τιμή_ιδιότητας_2#...#τιμή_ιδιότητας_n#". H συνάρτηση αυτή διαχωρίζει  κατάλληλα τη συμβολοσειρά str και αρχικοποιεί την δομή TuplePtr t περνώντας το όνομα_ιδιότητας και εισάγωντας διαδοχικά τις τιμές ιδιότητας στην generic λίστα της δομής.

3)Αν δεν ισχύει κάποια από τις περιπτώσεις 1) και 2) τότε το buffer περιέχει μία γραμμή στην οποία υπάρχει ένα ζεύγος (όνομα_ιδιότητας,τιμή_ιδιότητας).Τότε καλείται η συνάρτηση json_separator(char* buff,TuplePtr t) η οποία διαχωρίζει  κατάλληλα τη συμβολοσειρά str και αρχικοποιεί την δομή TuplePtr t περνώντας το όνομα_ιδιότητας και εισάγωντας την τιμή ιδιότητας στην generic λίστα της δομής. 

**!Διαχείση μη ταιριασμάτων**
Όταν διαβάσουμε από το dataset W ένα ζεύγος left_spec_id, right_spec_id τα όποια είναι ίδια (έχουν δηλαδή label να ισούται με 1), hash-άρουμε το κάθε προαναφερθέν id, βρίσκουμε σε ποιά θέση του hash table βρίσκεται και ύστερα με τη συνάρτηση foundInHT επιστρέφουμε το bucket στο οποίο ανήκει το συγκεκριμένο id καθώς και τη θέση που καταλαμβάνει στο bucket(δηλαδή ποιά εγγραφή είναι). Αφότου έχουμε τις ακριβείς θέσεις και των 2 spec id, καλούμε τη συνάρτηση changePointers η οποία αφότου ελέγξει ότι δεν αναφερόμαστε στην ίδια κλίκα(δηλαδή αποφεύγει φαινόμενα κυκλισμού), κάνει merge τις δύο κλίκες (λίστες) και σε κάθε στοιχείο της merged πια κλίκας πηγαίνει και αλλάζει τον pointer του προκειμένου όλα τα στοιχεία της κλίκας να δείχνουν(άρα και να αναφέρονται) στην ίδια κλίκα. 

**~Εκτύπωση αποτελέσματος**
Η εκτύπωση στο output.txt γίνεται όπως ακριβώς και στην πρώτη εργασία. Δημιουργείται επιπλέον ένα ακόμη αρχείο για τα αποτελέσματα του training με το όνομα result.txt. Κάθε γραμμή του περιέχει μια λέξη του vocabulary συνοδευόμενη από το τελικό βάρος της που προέκυψε μετά το training.

**Unit Testing**
Εφαρμόζουμε unit testing στις λίστες, τα τρία είδη hashTable, το wordInfo structure, και στις κύριες συναρτήσεις για τη δημιουργία των απαραίτητων για το training δομών. Οι μόνες συναρτήσεις που δεν ελέγχονται από το testing είναι μερικές από τις εκείνες που ασχολούνται με το input-output του προγράμματος, αυτές που αποδεσμεύουν μνήμη(αν και το πρόγραμμα δεν έχει κανένα leak ή error σύμφωνα με το Valgrind) και κάποιες μικρές συναρτήσεις εντός των δομών που ελέγχονται έμμεσα, αφού αποτελούν τμήμα μεγαλύτερων συναρτήσεων που ελέγχονται κανονικά. 
