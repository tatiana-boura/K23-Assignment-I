!: θέλει αλλαγή από 1η εργασία
~: θα μάθουμε τη τετάρτη

## PROJECT K23A Εργασία 2
	Χίου Ρίτα Άννα sdi1700192
	Χουσιανίτη Κατερίνα sdi1700194
	Μπούρα Τατιάνα sdi1700100

## Αρχεία και οργάνωση
Χωρίσαμε τα παραδοτέα τις εργασίας σε Project1, Project2 φακέλους και τα datasets συνεχίζουν να βρίσκονται σε δικό τους φάκελο(datasets). Να σημειωθεί ότι οι δύο φάκελοι Project1 , Project2 δεν περοέχουν ίδιο κώδικα.  Μέσα στον φάκελο Project2 θα βρείτε την main.c, το makefile, το παρόν README.md και το αρχείο common-english-words.txt καθώς και φακέλους οργανωμένους θεματικά είτε ανά δομή (π.χ. hash_table,list) είτε ανά ενότητα (π.χ. classification). 

## Compile and run
Για να τρέξει κατευθείαν από το repository στο Project2, χρησιμοποιώντας τα αρχεία στο φάκελο dataset υπάρχουν 2(δύο) τρόποι,

*1*
> make -C Project2

> ./Project2/prog ./datasets/camera_specs/2013_camera_specs ./datasets/sigmod_large_labelled_dataset.csv ./Project2/common-english-words.txt

					*ή*
> ./Project2/prog ./datasets/camera_specs/2013_camera_specs ./datasets/sigmod_medium_labelled_dataset.csv ./Project2/common-english-words.txt
  
*2*
> cd Project2

> make  

> ./prog ../datasets/camera_specs/2013_camera_specs/ ../datasets/sigmod_large_labelled_dataset.csv common-english-words.txt

					*ή*  					
> ./prog ../datasets/camera_specs/2013_camera_specs/ ../datasets/sigmod_medium_labelled_dataset.csv common-english-words.txt,

  
όπου sigmod_large_labelled_dataset.csv είναι το datasetW και το sigmod_medium_labelled_dataset το datasetY.  
  
Για το unit testing κάνουμε
> make tests  

όπου θα προκύψουν 6 εκτελέσιμα: 
- για τη δομή list
- για τη δομη hash_table όπου αποθηκεύονται όλες οι πληροφορίες κάθε .json αρχείου
- για τη δομη hash_table_pair που χρησιμοποιείται για τη δημιουργία των x_array, y_array για το training
- για τις δομές του vocabulary (hashTableVOC και λίστα)
- για τις συναρτήσεις που αφορούν τον υπολογισμό των tfidf, bag of words και τον επαναπροσδιορισμό του vocabulary
- για τη δομή wordInfo που χρησιμοποιείται για την αποθήκευση του περιεχομένου (λέξεων/αριθμού εμφανίσεων) του κάθε .json αρχείου
Για τα επιμέρους arguments του κάθε εκτελέσιμου ανατρέξατε στο TEST_LIST του κάθε .c του testing.

## Λειτουργικότητα / Σχεδιαστικές επιλογές

**Generic/Sorted List**
Οι generic λίστες μας φάνηκαν χρήσιμες για τις ανάγκες της πρώτης εργασίας (για τη δημιουργία κλικών από spec_ids και την διαχείριση του collision στα hashtable), αλλά και τις αλλαγές που προέκυψαν στην δεύτερη: για την συλλογή λέξεων (vocabulary) και την αποθήκευση των μη-κλικών, πάντα διατηρώντας την append from front συμπεριφορά για αποτελεσματικότερη εισαγωγή. 
Η sorted list αποσκοπεί στην γρήγορη πρόσβαση λέξεων στη wordInfoList (στο entry κάθε json του hash table) προς αποφυγή διπλοτύπων κατά την εισαγωγή νέων λέξεων. Να σημειωθεί ότι στο vocabulary που είναι και αυτό τύπου λίστας wordInfo χρησιμοποιούμε append front, αφού δε μας ενδιαφέρει η γρήγορη προσπέλαση (αυτή γίνεται με το hash που αναφέρεται παρακάτω) αλλά η γρήγορη εισαγωγή.

**Είδη Hash Table**
Σε αυτή την εργασία εντοπίσαμε την ανάγκη για 3 είδη hash table:
1. Η πρώτη μορφή είναι παρόμοια με αυτή της πρώτης εργασίας με τις εξής διαφορές:
- αντί για την λίστα από tuples (<όνομα_ιδιότητας,τιμές_ιδιότητας>), χρησιμοποιείται μια λίστα της δομής wordInfo (<λέξη,εμφανίσεις λέξης στο .json>) ώστε να μπορέσει αργότερα να υπολογιστεί το tfidf / bag of words
- προστέθηκε μια ακόμη λίστα, η notClique, η οποία αποτελεί λίστα από διευθύνσεις κλικών οι οποίες δεν ταιριάζουν με την κλίκα του συγκεκριμένου .json
- προστέθηκε ένας πίνακας από αριθμούς float που αποτελούν το tfidf / bag of words του .json ανάλογα με την επιλογή του χρήστη
2. Το δεύτερο hash table είναι αυτό που συγκρατεί τις θέσεις των λέξεων στο vocabulary και είναι τύπου hashTableVOC. Κάθε entry του αποτελείται από την λέξη, τη θέση της στη λίστα vocabulary και το στοιχείο wordInfo της λίστας. Αυτό μας επιτρέπει την γρήγορη δημιουργία των tfidf ή BoW vectors
3. Το τελευτείο είδος hash table που δημιουργήσαμε είναι το hashTablePair για την data_for_training. Αυτό το hash table αποτελείται από 2 spec_ids, τα οποία εισάγονται αφού χρησιμοποιηθούν ως ζεύγος στον πίνακα απόλυτων διαφορών για το training (x_array). Για την επιλογή επόμενων ζευγών, θα χρησιμοποιηθεί η foundInHTPair, η οποία θα ελέγξει αν τα 2 spec_ids υπάρχουν ήδη στο hash table (ως spec1,spec2 ή spec2,spec1). Αυτό μας επιτρέπει την ορθή δημιουργία του array που περιγράφεται πιο κάτω

**Δομή wordInfo**
Για τις ανάγκες υπολογισμού των tfidf / bag of words, δημιουργήθηκε η δομή wordInfo (<word,count>), η οποία αξιοποιείται στη δημιουργία του vocabulary και στη δομή hashTable. Η λίστα vocabulary, στην οποία αποθηκεύονται όλες οι λέξεις όλων των αρχείων .json, διαθέτει στοιχεία wordInfo* με word την εκάστοτε λέξη και count τον αριθμό των αρχείων json στα οποία υπάρχει. Στο hashTable, η λίστα wordInfoList αποτελείται από στοιχεία wordInfo* ωστόσο εδώ το count φυλάει το πλήθος εμφανίσεων της λέξης στο συγκεκριμένο αρχείο. 

**~Διάβασμα από dataset X**
Το πρόγραμμα για να εισάγει τα δεδομένα από το dataset X στις δομές που έχουμε επιλέξει σχεδιαστικά ακολουθεί την εξής διαδικασία:
Ανοίγει κάθε έναν από τους καταλόγους που αντιστοιχούν στις ιστοσελίδες διαδικτυακών καταστημάτων και επεξεργάζεται σειριακά όλα τα αρχεία .json που περιέχονται σε αυτούς.
Ολοκληρώνοντας την ανάγνωση του dataset X για κάθε αρχείο .json έχει μετατραπεί σε μία λίστα από δομές wordInfo έτσι ώστε να υποστηρίζει την μετέπειτα λειτουργία του προγράμματος.

Πιο συγκεκριμένα, για την μετατροπή κάθε αρχείου .json στη μορφή λίστας, καλείται ση συνάρτηση json_to_wordInfo_list(), στην οποία το πρόγραμμα διαβάζει το αρχείο γραμμή-γραμμή, με χρήση της συνάρτησης fgets(char *str, int n, FILE *stream), και στη συνέχεια ελέγχοντας το περιεχόμενο του buffer(str) συνεχίζει σε μία από τις παρακάτω ενέργειες:

1. Προσπερνάει τις γραμμές που περιέχουν τους χαρακτήρες '{' η '}', οι οποίοι και οριοθετούν τα περιεχόμενα του αρχείου, το πρόγραμμα προχωράει στην ανάγνωση της επόμενης γραμμής του αρχείου.

2. Διαχείριση εγγραφής της μορφής: "ιδιότητα": "τιμή"
Διαχωρίζει τα πεδία "ιδιότητα" και "τιμή". 
α)Αν η "τιμή"δεν είναι της μορφής 'yes'/'no', τότε στη λίστα των λέξεων εισάγονται* οι λέξεις του πεδίου "τιμή"
β)Αν η "τιμή" είναι 'yes', τότε στη λίστα των λέξεων εισάγονται* οι λέξεις του πεδίου "ιδιότητα"
γ)Αν η "τιμή" είναι 'no', τότε δεν εισάγεται στη λίστα κάποια λέξη

3. Διαχείριση εγγραφής της μορφής: "ιδιότητα": ["τιμή_1",\n "τιμή_2",\n ...,\n"τιμή_n"\n] -πίνακας τιμών
Διαχωρίζει τα πεδία "ιδιότητα" και "τιμή".
Εντοπίζει την ύπαρξη πίνακα τιμών καθώς η "τιμή" είναι ο χαρακτήρας '['
Εισάγει* τις λέξεις του πεδίου "ιδιότητα"
Στη συνέχεια προχωρά στην ανάγνωση γραμμών του αρχείου που περιέχουν τις τιμές του πίνακα και εισάγει* τις λέξεις των γραμμών αυτών στη λίστα των λέξεων. Η ανάγνωση τιμών του πίνακα τερματίζεται όταν το buffer περιέχει τον χαρακτήρα ']', που σηματοδοτεί το τέλος ενός πίνακα τιμών.

[Αξίζει να αναφέρουμε πως κάθε λέξη που εισάγεται* στη λίστα wordInfo εισάγεται παράλληλα στο Vocabulary που θα χρησιμοποιηθεί για την κατασκευή των vector στην μετέπειτα λειτουργία του προγράμματος.]

*Για την εισαγωγή λέξεων από μία συμβολοσειρά, 
Ι. επεξεργαζόμαστε την συμβολοσειρά με την κλήση της συνάρτησης str_preprocess_symbols(char* str) έτσι ώστε:
-να μην περιέχει σύμβολα (παραμένουν μόνο αριθμοί και γράμματα)
-όλα τα κεφαλαία γράμματα μετατρέπονται σε πεζά
ΙΙ. χωρίζοντας τη συμβολοσειρά σε λέξεις, με χρήση της strtok(), πριν την εισαγωγή μίας λέξης γίνεται έλεγχος ώστε:
-να μην είναι σκέτος αριθμός
-να μην ανήκει στη λίστα stopwords, η οποία δημιουργείται από το αρχείο 'common-english-words.txt' που παρέχει ο χρήστης στην εντολή εκτέλεσης του προγράμματος
-να έχει μήκος μεγαλύτερο του ενός χαρακτήρα

**!Διαχείση ταιριασμάτων και μη**
Όταν διαβάσουμε από το dataset W ένα ζεύγος left_spec_id, right_spec_id τα όποια είναι ίδια (έχουν δηλαδή label να ισούται με 1), hash-άρουμε το κάθε προαναφερθέν id, βρίσκουμε σε ποιά θέση του hash table βρίσκεται και ύστερα με τη συνάρτηση foundInHT επιστρέφουμε το bucket στο οποίο ανήκει το συγκεκριμένο id καθώς και τη θέση που καταλαμβάνει στο bucket(δηλαδή ποιά εγγραφή είναι). Αφότου έχουμε τις ακριβείς θέσεις και των 2 spec id, καλούμε τη συνάρτηση changePointers η οποία αφότου ελέγξει ότι δεν αναφερόμαστε στην ίδια κλίκα(δηλαδή αποφεύγει φαινόμενα κυκλισμού), κάνει merge τις δύο κλίκες (λίστες) και σε κάθε στοιχείο της merged πια κλίκας πηγαίνει και αλλάζει τον pointer του προκειμένου όλα τα στοιχεία της κλίκας να δείχνουν(άρα και να αναφέρονται) στην ίδια κλίκα. Παράλληλα κάνει merge δίχως διπλότυπα τα στοιχεία της αντικλίκας που έχουν παρατηρηθεί μέχρι στιγμής για την καθεμία από τις δύο κλίκες. Ομοίως, φροντίζει να μην υπάρχουν διπλότυπα στα στοιχεία των προαναφερθέντων αντικλικών εάν εξ αρχής οι δο κλίκες που συνενώνονται "δείχνουν" σε μια ίδια αντικλίκα.

Όταν διαβάσουμε από το dataset W ένα ζεύγος left_spec_id, right_spec_id τα όποια δεν είναι ίδια (έχουν δηλαδή label να ισούται με 0), hash-άρουμε το κάθε προαναφερθέν id, βρίσκουμε σε ποιά θέση του hash table βρίσκεται και ύστερα με τη συνάρτηση foundInHT επιστρέφουμε το bucket στο οποίο ανήκει το συγκεκριμένο id καθώς και τη θέση που καταλαμβάνει στο bucket(δηλαδή ποιά εγγραφή είναι). Αφότου έχουμε τις ακριβείς θέσεις και των 2 spec id, καλούμε τη συνάρτηση adjustPointers η οποία προσθέτει στη λίστα των αντικλικών του ενός spec id (και των στοιχείων που ανήκουν στην ίδια κλίκα με αυτό) την κλίκα του άλλου και αντίστροφα, χωρίς διπλότυπα.
 
**Δημιουργία πίνακα tfidf ή BoW και διαχωρισμός του σε train και validation dataset**
Με το που τελειώσει η δημιουργία του hash table στο οποίο αποθηκεύονται τα αρχεία, απαιτείται από τον χρήστη να πλήκτρολογήσει 't' εφόσον επιθυμεί το training να γίνει με tfidf representation ή 'b' εφόσον επιθυμεί το training να γίνει με BoW representation. Έπειτα, για κάθε αρχείο json δημιουργείται το δίανυσμα tfidf ή BoW (ανάλογα με το τι έχει επιλέξει) ως εξής:
- tfidf: Υπάρχουν δύο(2) συναρτήσεις γι' αυτό με μια μικρή διαφορά. 

Η πρώτη που ονομάζεται make_tfidf_vectorsDROP υπολογίζει αρχικά το μέσο tfidf της κάθε λέξης και ύστερα 'πετά' από το vocabulary τις λέξεις με μέσο tfidf < 0.0001*numOfJSON όπου numOfJSON ο συνολικός αριθμός των αρχείων json. Ύστερα 'πετά' και τις αντίστοιχες στήλες από το διάνυσμα tfidf του κάθε json. Η συνάρτηση αυτή δεν επαναϋπολογίζει τα tfidf. Η δεύτερη(make_tfidf_vectorsDROPnRECOMPUTE) λειτουργεί με τον ίδιο τρόπο, μόνο που το διάνυμα tfidf επαναϋπολογίζεται.

Και με τις δύο συναρτήσεις καταλήγουμε σε λεξιλόγιο μεγέθους 1824 words. Και οι δύο είναι πλήρως λειτουργικές σε περίπτωση που επιθυμεί ο εξεταστής να τις τρέξει, αλλά ως default έχουμε την δεύτερη. 

- BoW: Η συνάρτηση αυτή χρησιμοποιεί το μέσο tfidf για να κάνει drop columns. Και αυτή καταλήγει για κάθε json με ένα BoW vector με 1824 columns.

Αφότου διαβαστεί το αρχείο datasetW και δημιουργηθούν οι κλίκες και οι αντικλίκες, δημιουργούμε με τη συνάρτηση create_x_y_array έναν array X με τις απόλυτες διαφορές των tfidf ή BoW vectors των δύο εμπλεκόμενων json και έναν array Y που έχει τιμή 1 ή 0 εάν το ζεύγος ανήκει σε κλίκα ή αντικλίκα αντίστοιχα. Για το datasetW έχουμε 42535 ζευγή με αποτίμηση 1 και 299394 ζεύγη με αποτίμηση 0.

Έπειτα, με shuffling του παραπάνω array (δηλαδή τυχαία) δημιουργούμε τα training και validation sets με αναλογία (80-20). Να σημειωθεί ότι δε δεσμεύεται νέος χώρος για τις στήλες, συνεπώς η συνάρτηση αυτή (createSets) είναι πολύ memory efficient και γρήγορη αφού χρησιμοποιεί boolean array για την τυχαία διαλογή.


** Training και prediction του validation **
Με τον παραπάνω διαχωρισμό προχωράμε στο training , το οποίο γίνεται με stochastic gradient descent. Στο αντίστοιχο αρχείο logisticReg.c και στη συνάρτηση gradient_descent() υπάρχει και σχολιασμένος ο κώδικας για την non-stochastic gradient descent σε περίπτωση που ο εξεταστής επιθυμεί να την τρέξει.

Τέλος, γίνεται η πρόβλεψη για το validation set και δίνεται ένα ποσοστό επιτυχούς πρόβλεψης.

**~Εκτύπωση αποτελέσματος**
Η εκτύπωση στο output.txt γίνεται όπως ακριβώς και στην πρώτη εργασία. Δημιουργείται επιπλέον ένα ακόμη αρχείο για τα αποτελέσματα του training με το όνομα result.txt. Κάθε γραμμή του περιέχει μια λέξη του vocabulary συνοδευόμενη από το τελικό βάρος της που προέκυψε μετά το training.

**Unit Testing**
Εφαρμόζουμε unit testing στις λίστες, τα τρία είδη hashTable, το wordInfo structure, και στις κύριες συναρτήσεις για τη δημιουργία των απαραίτητων για το training δομών. Οι μόνες συναρτήσεις που δεν ελέγχονται από το testing είναι μερικές από τις εκείνες που ασχολούνται με το input-output του προγράμματος, αυτές που αποδεσμεύουν μνήμη(αν και το πρόγραμμα δεν έχει κανένα leak ή error σύμφωνα με το Valgrind) και κάποιες μικρές συναρτήσεις εντός των δομών που ελέγχονται έμμεσα, αφού αποτελούν τμήμα μεγαλύτερων συναρτήσεων που ελέγχονται κανονικά. 
